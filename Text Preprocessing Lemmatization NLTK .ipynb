{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6236ce78-e17e-453f-8e09-ec21b48521be",
   "metadata": {},
   "source": [
    "### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d7d88a-e916-4ae2-939a-13aee74f41de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "'''\n",
    "pos - Noun --> n\n",
    "verb --> v\n",
    "adjective --> a\n",
    "adverb --> r\n",
    "'''\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"going\",pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49cb6c26-361e-49ab-b49c-64213fe0e44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb942f17-b8cc-4daf-8e49-c1e8b8cd88e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\",pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5979f1ab-f8f8-4ab9-88eb-405b6f53deff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\",pos='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb82150d-5349-4f04-8eff-c5a4ace82657",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['eating','eats','eaten','writing','written','fairly','programming','programs','history','finally','finalize','sitting','congrats','congratulation','congratulating','sports','sportingly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76cc498-26fb-4bd9-938d-05171a71b1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------->eat\n",
      "eats------->eat\n",
      "eaten------->eat\n",
      "writing------->write\n",
      "written------->write\n",
      "fairly------->fairly\n",
      "programming------->program\n",
      "programs------->program\n",
      "history------->history\n",
      "finally------->finally\n",
      "finalize------->finalize\n",
      "sitting------->sit\n",
      "congrats------->congrats\n",
      "congratulation------->congratulation\n",
      "congratulating------->congratulate\n",
      "sports------->sport\n",
      "sportingly------->sportingly\n"
     ]
    }
   ],
   "source": [
    "for word in word_list:\n",
    "    print(word+\"------->\"+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f156f217-b324-451e-a6a3-a73d11aada33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------->eating\n",
      "eats------->eats\n",
      "eaten------->eaten\n",
      "writing------->writing\n",
      "written------->written\n",
      "fairly------->fairly\n",
      "programming------->programming\n",
      "programs------->programs\n",
      "history------->history\n",
      "finally------->finally\n",
      "finalize------->finalize\n",
      "sitting------->sitting\n",
      "congrats------->congrats\n",
      "congratulation------->congratulation\n",
      "congratulating------->congratulating\n",
      "sports------->sports\n",
      "sportingly------->sportingly\n"
     ]
    }
   ],
   "source": [
    "for word in word_list:\n",
    "    print(word+\"------->\"+lemmatizer.lemmatize(word,pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da8c642-5fbf-4ba7-a910-6849ac26e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "eats ----> eat\n",
      "eaten ----> eaten\n",
      "writing ----> write\n",
      "written ----> written\n",
      "fairly ----> fair\n",
      "programming ----> program\n",
      "programs ----> program\n",
      "history ----> histori\n",
      "finally ----> final\n",
      "finalize ----> final\n",
      "sitting ----> sit\n",
      "congrats ----> congrat\n",
      "congratulation ----> congratul\n",
      "congratulating ----> congratul\n",
      "sports ----> sport\n",
      "sportingly ----> sport\n",
      "Stemmed Words List: ['eat', 'eat', 'eaten', 'write', 'written', 'fair', 'program', 'program', 'histori', 'final', 'final', 'sit', 'congrat', 'congratul', 'congratul', 'sport', 'sport']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Create a new list to store the stemmed words\n",
    "stemmed_wordlist = []\n",
    "\n",
    "# Stem each word and add it to the new list\n",
    "for word in word_list:\n",
    "    stemmed_word = snowball_stemmer.stem(word)\n",
    "    stemmed_wordlist.append(stemmed_word)\n",
    "    print(word + \" ----> \" + stemmed_word)\n",
    "\n",
    "# Display the final list of stemmed words\n",
    "print(\"Stemmed Words List:\", stemmed_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b9742a-c9d3-471a-8c59-94e98ad6bd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat------->eat\n",
      "eat------->eat\n",
      "eaten------->eaten\n",
      "write------->write\n",
      "written------->written\n",
      "fair------->fair\n",
      "program------->program\n",
      "program------->program\n",
      "histori------->histori\n",
      "final------->final\n",
      "final------->final\n",
      "sit------->sit\n",
      "congrat------->congrat\n",
      "congratul------->congratul\n",
      "congratul------->congratul\n",
      "sport------->sport\n",
      "sport------->sport\n"
     ]
    }
   ],
   "source": [
    "for word in stemmed_wordlist:\n",
    "    print(word+\"------->\"+lemmatizer.lemmatize(word,pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4be2c-2219-42e9-8e77-aa7cdf1b0a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
